---
title: "First Exploration in Capstone Project"
author: "Michael Pearson"
date: "8/06/2017"
output:
  html_document: default
  pdf_document: default
---

###Let's begin by loading the libraries and reading the files on my computer where the Swiftkey zips have been unzipped. 
```{r get the data into tidy format, include=TRUE, message= FALSE, warning= FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidytext, quietly = TRUE)
library(dplyr, quietly = TRUE)
library(readr, quietly = TRUE)
library(R.utils, quietly = TRUE)
library(tm, quietly = TRUE)
library(SnowballC, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(ggraph, quietly = TRUE)
library(igraph, quietly = TRUE)
library(data.table, quietly = TRUE)
eng_news <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
eng_blogs <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
eng_twitter <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
blog_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
tweet_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
news_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
newslines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
bloglines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
tweetlines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
blog25 <- readLines(blog_us, 25)
close(blog_us)
tweet25 <- readLines(tweet_us, 25)
close(tweet_us)
news25 <- readLines(news_us, 25)
close(news_us)
```
# Exploring the data

##  Some basic examination of the three text sources: the News, the Blogs, the Tweets.

The news file has `r prettyNum(nchar(eng_news), big.mark=",", scientific = FALSE)` characters, and `r prettyNum(newslines, big.mark=",", scientific = FALSE)` lines of text. The news file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes.

The blog file has `r prettyNum(nchar(eng_blogs), big.mark=",", scientific = FALSE) ` characters, and `r prettyNum(bloglines, big.mark=",", scientific = FALSE)` of text. The blog file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes

The twitter file has `r prettyNum(nchar(eng_twitter), big.mark=",", scientific = FALSE)` characters and `r prettyNum(tweetlines, big.mark=",", scientific = FALSE)` of text. The twitter file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes


Beginning of text processing
```{r count chars, include=TRUE}
newzchar <- nchar(news25)
sumnew <- sum(newzchar)
tweetchar <- nchar(tweet25)
sumtweet <- sum(tweetchar)
blogchar <- nchar(blog25)
sumblog <- sum(blogchar)
```
## Begin the exploration by loading the full texts of the Swiftkey files...


```{r get the token, include=TRUE, warning= FALSE, message = FALSE}
tweet_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
tweet_all <- readLines(tweet_us, n= tweetlines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
close(tweet_us)
love_it <- length(grep("love", tweet_all))
hate_it <- length(grep("hate", tweet_all))
phrase_it <- length(grep("A computer once beat me at chess, but it was no match for me at kickboxing", tweet_all))
blog_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
blog_all <- readLines(blog_us, n= bloglines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
close(blog_us)
news_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
news_all <- readLines(news_us, n = newslines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
close(news_us)
long_tweet <- max(nchar(tweet_all[1:tweetlines]))
long_news <- max(nchar(news_all[1:newslines]))
long_blog <- max(nchar(blog_all[1:bloglines]))
```
## More exploration of the texts

Some of these are from the quiz for the first week of the Capstone Project...

The phrase "A computer once beat me at chess, but it was no match for me at kickboxing" occurs `r phrase_it` times in the twitter sample.

The word 'love' occurs `r prettyNum(love_it, big.mark = ",", scientific = FALSE)` times in the twitter sample.

The word 'hate' occurs `r prettyNum(hate_it, big.mark = ",", scientific = FALSE)` times in the twitter sample.

The longest line in the twitter sample is `r long_tweet` characters. Duh!

The longest line in the blog sample is `r prettyNum(long_blog, big.mark = ",", scientific = FALSE)` characters.

The longest line in the news sample is `r prettyNum(long_news, big.mark = ",", scientific = FALSE)` characters.

## Getting rid of the profanity


I got a list of profanity from Google: full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt 

I will use this file to filter profanity from my sample of the corpus.



## Now let's create a sample of 10% of the text, load it into a corpus. 

Then we will remove profanity (the 'badwords' file from Google), tidy the corpus using the 'tidytext' package - which follows tidy data procedures and makes one variable per column.

I use the file "full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt" to create a list of profanity to remove from the samples.


We will remove non alphabetic characters, remove blanks, and then count word frequencies, and create tidy data frames for bigrams and trigrams.

``` {r get the love, include=TRUE}
badwords <- readLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt", warn = FALSE)
set.seed(1151960)
samp_per <- 0.4
sam_twit <- tweet_all[sample(1:length(tweet_all),samp_per*length(tweet_all))]
##write_lines(sam_twit, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/twittersample.txt")
sam_news <- news_all[sample(1:length(news_all),samp_per*length(news_all))]
##write_lines(sam_news, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/newssample.txt")
sam_blog <- blog_all[sample(1:length(blog_all),samp_per*length(blog_all))]
##write_lines(sam_blog, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/blogsample.txt")
sam_Corpus <- VCorpus(DirSource("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples"))
sam_tidy <- tidy(sam_Corpus)
data("stop_words")
tidy_sentences <- data.table(sam_tidy) %>% unnest_tokens(sentences, text, token = "sentences")
text_tokens <- data.table(sam_tidy) %>% unnest_tokens(word, text, token = "words")
text_tokens$word <- gsub("[^[:alpha:] | ^[:punct:]]" , " ", text_tokens$word) 
text_tokens$word   <- gsub("-", " ", text_tokens$word)
tidy_sentences$sentences <- gsub("[^[:alpha:] | ^[:punct:]]", " ", tidy_sentences$sentences) 
tidy_sentences$sentences <- gsub("-", " ", tidy_sentences$sentences)
tidy_sentences$sentences <- gsub("  ", " ", tidy_sentences$sentences) 
text_tokens <- subset(text_tokens, word !="")
word_sen <- tidy_sentences %>% unnest_tokens(word, sentences) %>% anti_join(stop_words, by = "word")
text_count <- text_tokens %>% count(word, sort = TRUE)
rm(text_tokens)
write.csv(text_count, file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/text_count.csv")
rm(text_count)
word_count <- word_sen %>% count(word, sort = TRUE)
write.csv(word_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/word_count.csv" )
rm(word_count)
text_bigrams <- sam_tidy %>% unnest_tokens(bigram, text, token = "ngrams", n=2)
bigram_count <- text_bigrams %>% count(bigram, sort = TRUE)
rm(text_bigrams)
write.csv(bigram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/bigram_count.csv" )
rm(bigram_count)
sen_bigrams <- tidy_sentences %>% unnest_tokens(bigrams, sentences, token = "ngrams", n = 2)
sen_bigram_count <- sen_bigrams %>% count(bigrams, sort = TRUE)
rm(sen_bigrams)
write.csv(sen_bigram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_bigram_count.csv" )
rm(sen_bigram_count)
text_trigrams <- sam_tidy %>% unnest_tokens(trigram, text, token = "ngrams", n=3)
trigram_count <- text_trigrams %>% count(trigram, sort = TRUE)
rm(text_trigrams)
write.csv(trigram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/trigram_count.csv" )
rm(trigram_count)
sen_trigrams <- tidy_sentences %>% unnest_tokens(trigrams, sentences, token = "ngrams", n = 3)
sen_trigram_count <- sen_trigrams %>% count(trigrams, sort = TRUE)
rm(sen_trigrams)
write.csv(sen_trigram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_trigram_count.csv" )
rm(sen_trigram_count)
text_quadgrams <- sam_tidy %>% unnest_tokens(quadgram, text, token = "ngrams", n=4)
quadgram_count <- text_quadgrams %>% count(quadgram, sort = TRUE)
rm(text_quadgrams)
write.csv(quadgram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/quadgram_count.csv" )
rm(quadgram_count)
sen_quadgrams <- tidy_sentences %>% unnest_tokens(quadgrams, sentences, token = "ngrams", n = 4)
sen_quadgram_count <- sen_quadgrams %>% count(quadgrams, sort = TRUE)
rm(sen_quadgrams_count)
write.csv(sen_quadgram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_quadgram_count.csv" )
rm(sen_quadgram_count)
text_quingrams <- sam_tidy %>% unnest_tokens(quingram, text, token = "ngrams", n=5)
quingram_count <- text_quingrams %>% count(quingram, sort = TRUE)
rm(text_quingrams)
write.csv(quingram_count ,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/quingram_count.csv" )
rm(quingram_count)
sen_quingrams <- tidy_sentences %>% unnest_tokens(quingrams, sentences, token = "ngrams", n = 5)
sen_quingram_count <- sen_quingrams %>% count(quingrams, sort = TRUE)
rm(sen_quingrams)
write.csv(sen_quingram_count,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_quingram_count.csv" )
rm(sen_quingram_count)
```

## Count the words

Take a quick look at the head of the text count table.


```{r count the words and d, include=TRUE, eval= FALSE}
head(text_count)

```
## Addressing some of the review criteria...


Find the number of words that would cover half the lexicon

Find the number of words that would cover 90% of it.

Total words are `r prettyNum(sum(text_count$n), big.mark=",", scientific = FALSE)`

Half that is `r prettyNum(sum(text_count$n)/2, big.mark = ",", scientific = FALSE)`

I will now do the calculation to determine both of the above benchmarks.

```{r half way point, include=TRUE, eval= FALSE }
text_count$sum <- cumsum(text_count$n)
fiddy <- which(text_count$sum > sum(text_count$n/2))
ninety <- which(text_count$sum > sum(text_count$n)*0.9)
```
## Needed words are...

It would take the most frequent `r fiddy[1]` words to cover 50% of all word instances of the `r prettyNum(sum(text_count$n), big.mark=",", scientific = FALSE)` words in the sample corpus.

It would take `r ninety[1]` words to cover 90% of all word instances in the sample corpus.


## Let's do some graphing, plotting and histogramming

###Here's a bar plot of the 25 most common words in the sample

```{r Word Count plot, include=TRUE, eval= FALSE}
samp_plot <- text_count[1:25,]
g <- ggplot(data = samp_plot, aes(x=factor(word,levels = word),  y= n)) + geom_bar(stat="identity", fill = "green")
g<- g + theme(axis.text.x = element_text(angle = 90))
g
```
## Now let's bar plot the top 25 most frequent bigrams



```{r bigram plot, eval= FALSE}
bigram_count[1:25,]
bigram_samp <- bigram_count[1:25,]
bigram_plot <- ggplot(data = bigram_samp, aes(x=factor(bigram, levels=bigram), y= n)) + geom_bar(stat="identity", fill = "blue")
bigram_plot <- bigram_plot + theme(axis.text.x = element_text(angle = 90))
bigram_plot
```
## And now let's bar plot the top 25 most frequent trigrams


```{r trigram plot, eval= FALSE}
trigram_count[1:25,]
trigram_samp <- trigram_count[1:25,]
trigram_plot <- ggplot(data = trigram_samp, aes(x=factor(trigram, levels = trigram), y= n)) + geom_bar(stat="identity", fill = "blue")
trigram_plot <- trigram_plot + theme(axis.text.x = element_text(angle = 90))
trigram_plot
```
# Word Clouds

Let's make a word cloud of the top 50 words in the sample corpus
```{r word cloud of 25 words, eval= FALSE}
library(wordcloud)
text_tokens %>% count(word) %>% with(wordcloud(word, n, max.words = 50, rot.per = 0.55, colors = brewer.pal(8, "Dark2")))
```
## Bigram word clouds

Let's make a wordcloud using the top 25 bigrams
```{r bigram wordcloud, eval= FALSE}
bigram_count  %>% with(wordcloud(bigram, n, max.words = 25, rot.per = 0.45, colors = brewer.pal(7, "Spectral")))
```
## Trigram word clouds

Let's make a wordcloud of the top 25 trigrams
```{r trigram wordcloud, eval= FALSE}
trigram_count  %>% with(wordcloud(trigram, n, max.words = 25, rot.per = 0.35, colors = brewer.pal(11, "Paired")))
```
## Make a Markov chain of a few parts.


```{r chains, message= FALSE, warning= FALSE, eval= FALSE}
bi_bigrams <- separate(bigram_count, bigram, c("word1", "word2"), sep = " ")
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigram_graph <- bi_bigrams %>% filter(n > 2000) %>% graph_from_data_frame()
ggraph(bigram_graph, layout = "fr") + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) + geom_node_point(color = "lightblue", size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1)
 theme_void()
tri_trigrams <- separate(trigram_count, trigram, c("word1", "word2", "word3"), sep = " ")
quad_quadgrams <-  separate(quadgram_count, quadgram, c("word1", "word2", "word3", "word4"), sep = " ")
quin_quingrams <-  separate(quingram_count, quingram, c("word1", "word2", "word3", "word4", "word5"), sep = " ")
write.csv(bi_bigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/bi_bigrams.csv" )
write.csv(tri_trigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/tri_trigrams.csv" )
write.csv(quad_quadgrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/quad_quadgrams.csv" )
write.csv(quin_quingrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/quin_quingrams.csv" )

```
## Play with some text Dog.

Bring together all but the n'th gram. Then use data.table functions to go fast

and you can use the setorder command in data.table (and -n for descending )
```{r make it small, eval= FALSE}
bi_bigrams <- data.table(bi_bigrams)
tri_trigrams <- data.table(tri_trigrams)
quad_quadgrams <- data.table(quad_quadgrams)
quin_quingrams <- data.table(quin_quingrams)
setkey(bi_bigrams,word1)
combi_tri <- unite(tri_trigrams, bigrams, c("word1", "word2"), sep = " ")
combi_quad <- unite(quad_quadgrams, trigrams, c("word1", "word2", "word3"), sep = " ")
combi_quin <- unite(quin_quingrams, quadgrams, c("word1", "word2","word3", "word4"), sep = " ")
write.csv(combi_tri,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/combi_trigrams.csv" )
write.csv(combi_quad,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/combi_quadgrams.csv" )
write.csv(combi_quin,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/combi_quingrams.csv")

setkey(combi_tri, bigrams)
setkey(combi_quad, trigrams)
setkey(combi_quin, quadgrams)
##example
Do_it_is <- combi_tri["with one"]
Do_another <- combi_quad["all or nothing"]
Do_more <- combi_quin["at the intersection of"]
print(Do_it_is)
print(Do_another)
print(Do_more)
```