---
title: "New_First_Analysis"
author: "Michael Pearson"
date: "12/20/2017"
output:
  html_document: default
  pdf_document: default
---

###Keep the n gram files - hadn't done that before. 
```{r get the data into tidy format, include=TRUE, message= FALSE, warning= FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidytext, quietly = TRUE)
library(dplyr, quietly = TRUE)
library(readr, quietly = TRUE)
library(R.utils, quietly = TRUE)
library(tm, quietly = TRUE)
library(SnowballC, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(data.table, quietly = TRUE)
eng_news <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
eng_blogs <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
eng_twitter <- read_file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
blog_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
tweet_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
news_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
newslines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
bloglines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
tweetlines <- countLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
blog25 <- readLines(blog_us, 25)
close(blog_us)
tweet25 <- readLines(tweet_us, 25)
close(tweet_us)
news25 <- readLines(news_us, 25)
close(news_us)
```
# Exploring the data

##  Some basic examination of the three text sources: the News, the Blogs, the Tweets.

The news file has `r prettyNum(nchar(eng_news), big.mark=",", scientific = FALSE)` characters, and `r prettyNum(newslines, big.mark=",", scientific = FALSE)` lines of text. The news file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes.

The blog file has `r prettyNum(nchar(eng_blogs), big.mark=",", scientific = FALSE) ` characters, and `r prettyNum(bloglines, big.mark=",", scientific = FALSE)` of text. The blog file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes

The twitter file has `r prettyNum(nchar(eng_twitter), big.mark=",", scientific = FALSE)` characters and `r prettyNum(tweetlines, big.mark=",", scientific = FALSE)` of text. The twitter file is `r prettyNum(file.size("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")/(1024*1024), big.mark=",", scientific = FALSE)` MegaBytes


Beginning of text processing
```{r count chars, include=TRUE}
newzchar <- nchar(news25)
sumnew <- sum(newzchar)
tweetchar <- nchar(tweet25)
sumtweet <- sum(tweetchar)
blogchar <- nchar(blog25)
sumblog <- sum(blogchar)
```
## Begin the exploration by loading the full texts of the Swiftkey files...


```{r get the token, include=TRUE, warning= FALSE, message = FALSE}
tweet_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.twitter.txt")
tweet_all <- readLines(tweet_us, n= tweetlines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
close(tweet_us)
love_it <- length(grep("love", tweet_all))
hate_it <- length(grep("hate", tweet_all))
phrase_it <- length(grep("A computer once beat me at chess, but it was no match for me at kickboxing", tweet_all))
blog_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.blogs.txt")
blog_all <- readLines(blog_us, n= bloglines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
close(blog_us)
news_us <- file("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/en_US/en_US.news.txt")
news_all <- readLines(news_us, n = newslines, warn = FALSE, encoding = "UTF=8", skipNul = TRUE)
romance <- length(grep("romantic date", news_all))
close(news_us)
long_tweet <- max(nchar(tweet_all[1:tweetlines]))
long_news <- max(nchar(news_all[1:newslines]))
long_blog <- max(nchar(blog_all[1:bloglines]))
```
## More exploration of the texts

Some of these are from the quiz for the first week of the Capstone Project...

The phrase "A computer once beat me at chess, but it was no match for me at kickboxing" occurs `r phrase_it` times in the twitter sample.

The word 'love' occurs `r prettyNum(love_it, big.mark = ",", scientific = FALSE)` times in the twitter sample.

The word 'hate' occurs `r prettyNum(hate_it, big.mark = ",", scientific = FALSE)` times in the twitter sample.

The longest line in the twitter sample is `r long_tweet` characters. Duh!

The longest line in the blog sample is `r prettyNum(long_blog, big.mark = ",", scientific = FALSE)` characters.

The longest line in the news sample is `r prettyNum(long_news, big.mark = ",", scientific = FALSE)` characters.

The phrase "romantic date" occurs `r romance` times.

## Getting rid of the profanity


I got a list of profanity from Google: full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt 

I will use this file to filter profanity from my sample of the corpus.



## Now let's create a sample of 50% of the text, load it into a corpus. 

Then we will remove profanity (the 'badwords' file from Google), tidy the corpus using the 'tidytext' package - which follows tidy data procedures and makes one variable per column.

I use the file "full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt" to create a list of profanity to remove from the samples.


We will remove non alphabetic characters, remove blanks, and then count word frequencies, and create tidy data frames for bigrams and trigrams.

``` {r get the corpus, include= TRUE, eval= TRUE}
badwords <- readLines("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt", warn = FALSE)
set.seed(1151960)
samp_per <- 0.5
sam_twit <- tweet_all[sample(1:length(tweet_all),samp_per*length(tweet_all))]
write_lines(sam_twit, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/twittersample.txt")
sam_news <- news_all[sample(1:length(news_all),samp_per*length(news_all))]
write_lines(sam_news, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/newssample.txt")
sam_blog <- blog_all[sample(1:length(blog_all),samp_per*length(blog_all))]
write_lines(sam_blog, "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples/blogsample.txt")
sam_Corpus <- VCorpus(DirSource("/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/files/samples"))
sam_tidy <- tidy(sam_Corpus)
data("stop_words")
```
Two versions of the corpus are loaded

```{r clean the corpus, include= TRUE, eval= TRUE}
tidy_sentences <- data.table(sam_tidy) %>% unnest_tokens(sentences, text, token = "sentences")
text_tokens <- data.table(sam_tidy) %>% unnest_tokens(word, text, token = "words")
text_tokens$word <- gsub("[^[:alpha:] | ^[:punct:]]" , " ", text_tokens$word) 
text_tokens$word   <- gsub("-", " ", text_tokens$word)
tidy_sentences$sentences <- gsub("[^[:alpha:] | ^[:punct:]]", " ", tidy_sentences$sentences) 
tidy_sentences$sentences <- gsub("-", " ", tidy_sentences$sentences)
tidy_sentences$sentences <- gsub("  ", " ", tidy_sentences$sentences) 
```

```{r get the bigrams, include= TRUE, eval= TRUE}
text_bigrams <- sam_tidy %>% unnest_tokens(bigram, text, token = "ngrams", n=2) 
text_bigrams <- text_bigrams$bigram
write.csv(text_bigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/text_bigrams.csv" )
rm(text_bigrams)
sen_bigrams <- tidy_sentences %>% unnest_tokens(bigrams, sentences, token = "ngrams", n = 2)
sen_bigrams <- sen_bigrams$bigrams
write.csv(sen_bigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_bigrams.csv" )
rm(sen_bigrams)
```
## Now the trigrams

```{r get the trigrams, include= TRUE, eval= TRUE}
text_trigrams <- sam_tidy %>% unnest_tokens(trigram, text, token = "ngrams", n=3)
text_trigrams <- text_trigrams$trigram
write.csv(text_trigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/text_trigrams.csv" )
rm(text_trigrams)
sen_trigrams <- tidy_sentences %>% unnest_tokens(trigrams, sentences, token = "ngrams", n = 3)
sen_trigrams <- sen_trigrams$trigrams
write.csv(sen_trigrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_trigrams.csv" )
rm(sen_trigrams)
```

## and the quadgrams
```{r get the quadgrams, include= TRUE, eval= TRUE}
text_quadgrams <- sam_tidy %>% unnest_tokens(quadgram, text, token = "ngrams", n=4)
text_quadgrams <- text_quadgrams$quadgram
write.csv(text_quadgrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/text_quadgrams.csv" )
rm(text_quadgrams)
sen_quadgrams <- tidy_sentences %>% unnest_tokens(quadgrams, sentences, token = "ngrams", n = 4)
sen_quadgrams <- sen_quadgrams$quadgrams
write.csv(sen_quadgrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_quadgrams.csv" )
rm(sen_quadgrams)
```

# and the quingrams

```{r lastly the quingrams, include= TRUE, eval= TRUE}
text_quingrams <- sam_tidy %>% unnest_tokens(quingram, text, token = "ngrams", n=5)
text_quingrams <- text_quingrams$quingram
write.csv(text_quingrams ,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/text_quingrams.csv" )
rm(text_quingrams)
sen_quingrams <- tidy_sentences %>% unnest_tokens(quingrams, sentences, token = "ngrams", n = 5)
sen_quingrams <- sen_quingrams$quingrams
write.csv(sen_quingrams,file = "/Users/mutecypher/Documents/Documents - Michael’s iMac/Coursera/Capstone Project/processed/sen_quingrams.csv" )
rm(sen_quingrams)
```


## and let's see how that goes